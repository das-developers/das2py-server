#!/usr/bin/env python3
"""Expand source definitions into final form based on the server configuration"""

import sys
import os.path
from os.path import join as pjoin
from os.path import dirname as dname
import optparse
import json
from io import StringIO

g_sConfPath = REPLACED_ON_BUILD

das2 = None  # Namespace anchor for das2 module, loaded after sys.path is set
             # via the config file

# ########################################################################## #
# Work around ubuntu apport bugs
if sys.excepthook != sys.__excepthook__:
	if sys.excepthook.__name__ == 'apport_excepthook':
		sys.stderr.write("Info: disabling Ubuntu's Apport hook\n")
		sys.excepthook = sys.__excepthook__
	else:
		sys.stderr.write("Warning: 3rd party exception hook is active\n")

# ########################################################################## #
# handle output
try:
	unicode
except NameError:
	unicode = str

def pout(item):
	"""If input item is bytes, write them, if item is a unicode string encode as
	utf-8 first"""	
	if isinstance(item, str):
		sys.stdout.buffer.write(item.encode('utf-8'))
		sys.stdout.buffer.write('\n'.encode('utf-8'))
	else:
		sys.stdout.buffer.write(item)
			
def perr(item):
	"""If input item is bytes, write them, if item is a unicode string encode as
	utf-8 first"""	
	if isinstance(item, str):
		sys.stderr.buffer.write(item.encode('utf-8'))
		sys.stderr.buffer.write('\n'.encode('utf-8'))
	else:
		sys.stderr.buffer.write(item)


class BufferLog(object):
	def __init__(self):
		self.fOut = StringIO()

	def write(self, sThing):
		self.fOut.write("%s\n"%sThing)

	def getvalue(self):
		return self.fOut.getvalue()

# ########################################################################## #
# Get my config file, boiler plate that has to be re-included in each script
# since the location of the modules can be configured in the config file

def getConf():
	
	if not os.path.isfile(g_sConfPath):
		if os.path.isfile(g_sConfPath + ".example"):
			perr(u"Move\n   %s.example\nto\n   %s\nto enable your site\n"%(
				  g_sConfPath, g_sConfPath))
		else:
			perr(u"%s is missing\n"%g_sConfPath)
			
		return None

	fIn = open(g_sConfPath, 'r')
	
	dConf = {}
	nLine = 0
	for sLine in fIn:
		nLine += 1
		iComment = sLine.find('#')
		if iComment > -1:
			sLine = sLine[:iComment]
	
		sLine = sLine.strip()
		if len(sLine) == 0:
			continue
		
		iEquals = sLine.find('=')
		if iEquals < 1 or iEquals > len(sLine) - 2:
			preLoadError(u"Error in %s line %d"%(g_sConfPath, nLine))
			fIn.close()
			return None
		
		sKey = sLine[:iEquals].strip()
		sVal = sLine[iEquals + 1:].strip(' \t\v\r\n\'"')
		dConf[sKey] = sVal
	
	fIn.close()
	
	# As a final step, inclued a reference to the config file itself
	dConf['__file__'] = g_sConfPath
	
	return dConf

# ########################################################################## #
# Update sys.path, boiler plate code that has to be re-included in each script
# since config file can change module path

def setModulePath(dConf):
	if 'MODULE_PATH' not in dConf:
		perr(u"Set MODULE_PATH = /dir/containing/das2server_python_module")
		return False	
	
	lDirs = dConf['MODULE_PATH'].split(os.pathsep)
	for sDir in lDirs:
		if os.path.isdir(sDir):
				if sDir not in sys.path:
					sys.path.insert(0, sDir)
		
	return True

# ########################################################################## #
# Writing files #

def writeFile(sPath, sOutput):
	#perr("Writing: %s"%sPath)
	sDir = dname(sPath)

	if not os.path.isdir(sDir):
		os.makedirs(sDir)

	with open(sPath, 'w') as f:
		f.write(sOutput)

def writeJsonFile(sPath, dOutput):
	sOutput = json.dumps(dOutput, indent="  ");
	writeFile(sPath, sOutput)


# ########################################################################## #
# Update the data source collection #

def _updateCollection(sPath, lOutput, bOverwrite):
	"""Create or update the source collection file at sPath.  Source collections
	define a list of sources that basically return the same data but do so
	via different methods.  They are typed by the convention, the following
	conventions are known:

		das/2.2 ---> dsdf.d2t
	   das/3.0 ---> The federated catalog httpstreamsrc object (required)
	   das-websock/1.0 --> The federted catalog websocksrc object
	   hapi/2.0 ---> Output of das2_hapi -d source.dsdf -i -n
	"""

	# First read the stream source to get basic coordinate and data info

	for sSource in lOutput:
		if bname(sSource) == 'dsdf.d2t':,'query.json','socket.json'):
			continue


# ########################################################################## #
# The program needs way better help than the default OptionParser can provide

class MyOptParse(optparse.OptionParser):
	def print_help(self, file=None):
		if file == None:
			file = sys.stdout
	
		file.write("""
NAME:
   das2_srv_sdef - Create a data source definition from flexible inputs

SYNOPSIS:
   das2_srv_sdef [options] FILE CATEGORY

DESCRIPTION:
   das_srv_sdef defines a data source for a server.  A single input FILE in 
   either *.dsdf or *.json syntax is parsed to produced the output.  The
   output consists of at least three files:

      dsdf.d2t - A das2 source description for use by traditional clients
      query.json - An HttpStreamSrc Das Federated Catalog object
      internal.json - Internal processing instructions for the server.

   Other outputs may be added to the basic forms above.

   The CATEGORY argument defines the relative path of the data source from
   the server root.  Typically categories are used to organized sources by
   mission name and then by instrument name, but this is merely a convertion.

   To create sources directly under the server root use '/' for the category.

OPTIONS:
   -h, --help  Print this help message and exit
	
   -c FILE, --config=FILE
               Use FILE as the das2server.conf configuration instead of the
               compiled in default.

   -i, --install
               Install the source definition under the datasources directory
               for the server defined by the configuration file.

   -e, --erase-coll
               Write only the new sources into the source collection file 
               overwriting any that are already present.

   -w, --web-sock
               In addition to an HttpStreamSrc object, also output a WebSockSrc
               object to a file named 'socket.json'.  This requires that 
               $WEBSOCK_URI is defined in the server configuration file.

   --no-gen
               When processing JSON source templates, only expand $include
               sections, don't expand automatically $generate'd definitions.
               This is incompatable with --install

FILES:
   Each das2py-server is defined by a single top-level configuration file.
   By default the configuration data for this program are taken from:
	
      %s

EXAMPLES:
   1. Process a das2 DSDF file for the Survey dataset from the Waves instrument
      on the Juno mission and produce output in the current directory:

         das2_srv_sdef survey.dsdf /Juno/WAV/Survey

   2. Process a JSON template for the Waveform dataset from the PWS instrument
      on Voyager 1 and install it for use by the server:

         das2_srv_sdef -i waveform.json /Voyager/1/PWS/Waveform

      The resulting data source will be visible at:

         $SERVER_URL/source/voyager/1/pws/waveform

      and also at:

         $SERVER_URL?server=dsdf&dataset=Voyager/1/PWS/Waveform         

      if it's compatable with the das2 API.  In the paths above, $SERVER_URL
      and  are defined by the server configuration file.

SEE ALSO:
   The dsdf format is defined by das2 ICD at DOI: 10.5281/zenodo.3588534

   The json template format is yet to be codified, see examples distributed
   with das2py-server.

"""%g_sConfPath)

# ########################################################################## #
def main(argv):
	global das2

	sUsage = "das2_srv_sdef [options] DATA_SOURCE_FILE"
	psr = MyOptParse(prog="das2_srv_sdef", usage="sUsage")

	psr.add_option('-c', '--config', dest="sConfig", default=g_sConfPath)
	psr.add_option('-i', '--install', action="store_true", dest="bInstall", default=False)
	psr.add_option('-e', '--erase-coll', action="store_true", dest=bOverColl, defalut=False)
	psr.add_option('-w', '--web-sock', action="store_true", dest='bSocSrc', default=False)
	psr.add_option('', '--no-gen', action="store_true", dest="bIncOnly", default=False)

	(opts,lArgs) = psr.parse_args()

	if len(lArgs) < 1:
		perr("No data source file specified, use -h for help.")
		return 13
	if len(lArgs) < 2:
		perr("No category specified, use -h for help.")
	else:
		sPath = lArgs[0]
		sCategory = lArgs[1]
	
	# Check the input type
	sInType = None
	if sPath.lower().endswith('.dsdf'):
		sInType = 'dsdf'
	else:
		sInType = 'json'
		if not sPath.lower().endswith('.json'):
			perr(
				"File %s is not recognized, does not end in one of '.dsdf' or '.json'"%sPath
			)
			return 14
	
	dConf = getConf()
	if dConf == None:
		return 17
		
	# Set the system path
	if not setModulePath(dConf):
		return 18
		
	# Load the das2 module
	try:
		das2 = __import__('das2', globals(), locals(), [], 0)
	except ImportError as e:
		perr("Error importing module 'das2' using %s\r\n: %s\n"%(
			str(e), opts.sConfig))
		return 19
	
	# Load the das2server.util module
	try:
		mTmp = __import__('das2server', globals(), locals(), ['util'], 0)
	except ImportError as e:
		perr("Error importing module 'das2server' using %s\r\n: %s\n"%(
			str(e), opts.sConfig))
		return 19
	try:
		U = mTmp.util
	except AttributeError:
		perr("Server definition: %s"%opts.sConfig)
		perr('No module named das2server.util under %s\n'%dConf['MODULE_PATH'])
		return 20

	fLog = BufferLog()

	if not os.path.isfile(sPath):
		perr('File %s does not exist'%sPath)
		return 21

	if opts.bSocSrc:
		if 'WEBSOCKET_URI' not in dConf:
			perr('Set WEBSOCKET_URI in %s to support realtime readers'%opts.sConfig);
			return 22
	
		if not U.convdsdf.hasRtSupport(fLog, dConf, sPath):
			perr('Not web-socket capable, Keyword "realTime" does not evaluate '+\
			     'to True in %s.'%sPath)
			return 22

	if opts.bInstall:
		if sCategory in ('\\', '/'):
			sOutDir = dConf['DATASRC_ROOT']
		else:
			sOutDir = pjoin(dConf['DATASRC_ROOT'], sCategory)

		perr("Destination: %s"%sOutDir)
	else:
		sOutDir = '.'

	# Now for input/output switch
	sOutApi = pjoin(sOutDir, 'query.json');
	sOutWs  = pjoin(sOutDir, 'socket.json')
	sOutInt = pjoin(sOutDir, 'internal.json');
	sOutDas1 = pjoin(sOutDir, U.formats.g_sDas1File)
	sOutD2t = pjoin(sOutDir, 'dsdf.d2t');

	perr("Input:  %s %s"%(opts.sConfig, sPath))
	lOutput = []
	try:
		if sInType == 'dsdf':
			writeJsonFile(sOutApi, U.convdsdf.makeGetSrc(fLog, dConf, sPath))
			lOutput.append(sOutApi)
			
			if opts.bSocSrc:
				writeJsonFile(sOutWs, U.convdsdf.makeSockSrc(fLog, dConf, sPath))
				lOutput.append(sOutWs)

			writeJsonFile(sOutInt, U.convdsdf.makeInternal(fLog, dConf, sPath))
			lOutput.append(sOutInt)

			writeFile(sOutD2t, U.convdsdf.makeD2t(fLog, dConf, sPath))
			lOutput.append(sOutD2t)

			sDas1 = U.convdsdf.makeDas1(fLog, dConf, sPath)
			if sDas1: 
				writeFile(sOutDas1, sDas1)
				lOutput.append(sOutDas1)				
				
			perr("Output: %s"%(" ".join(lOutput)))
		else:
			writeJsonFile(sOutApi, U.convjson.makeFedCat(fLog, dConf, sPath))
			writeJsonFile(sOutInt, U.convjson.makeDas3Int(fLog, dConf, sPath))
			writeFile(sOutD2t, U.convjson.makeD2t(fLog, dConf, sPath))

	# Read the sources you've written and update the collection
	_updateCollection(pjoin(sOutDir, 'sources.json'), lOutput, opts.bOverColl)

	except U.errors.ServerError as e:
		perr('%s'%fLog.getvalue())
		perr('ERROR: %s'%str(e))
		return 22

	return 0

# ########################################################################## #
if __name__ == '__main__':
	main(sys.argv)
