#!/usr/bin/env python3
"""Expand source definitions into final form based on the server configuration"""

import sys
import os.path
from os.path import join as pjoin
from os.path import dirname as dname
from os.path import basename as bname
import optparse
import json
from io import StringIO

g_sConfPath = REPLACED_ON_BUILD

U = None  # Namespace anchor for das2server.util module, loaded after sys.path 
          # is set via the config file

# ########################################################################## #
# Work around ubuntu apport bugs
if sys.excepthook != sys.__excepthook__:
	if sys.excepthook.__name__ == 'apport_excepthook':
		#sys.stderr.write("Info: disabling Ubuntu's Apport hook\n")
		sys.excepthook = sys.__excepthook__
	else:
		sys.stderr.write("Warning: 3rd party exception hook is active\n")

# ########################################################################## #
# handle output
			
def perr(item):
	"""If input item is bytes encode as utf-8 first"""	
	if isinstance(item, str):
		sys.stderr.buffer.write(item.encode('utf-8'))
		sys.stderr.buffer.write('\n'.encode('utf-8'))
	else:
		sys.stderr.buffer.write(item)

class BufferLog(object):
	def __init__(self):
		self.fOut = StringIO()

	def write(self, sThing):
		self.fOut.write("%s\n"%sThing)

	def getvalue(self):
		return self.fOut.getvalue()

# ########################################################################## #
# Get my config file, boiler plate that has to be re-included in each script
# since the location of the server module is in the config file, not sys.path

def getConf():
	
	if not os.path.isfile(g_sConfPath):
		if os.path.isfile(g_sConfPath + ".example"):
			perr(u"Move\n   %s.example\nto\n   %s\nto enable your site\n"%(
				  g_sConfPath, g_sConfPath))
		else:
			perr(u"%s is missing\n"%g_sConfPath)
			
		return None

	fIn = open(g_sConfPath, 'r')
	
	dConf = {}
	nLine = 0
	for sLine in fIn:
		nLine += 1
		iComment = sLine.find('#')
		if iComment > -1:
			sLine = sLine[:iComment]
	
		sLine = sLine.strip()
		if len(sLine) == 0:
			continue
		
		iEquals = sLine.find('=')
		if iEquals < 1 or iEquals > len(sLine) - 2:
			preLoadError(u"Error in %s line %d"%(g_sConfPath, nLine))
			fIn.close()
			return None
		
		sKey = sLine[:iEquals].strip()
		sVal = sLine[iEquals + 1:].strip(' \t\v\r\n\'"')
		dConf[sKey] = sVal
	
	fIn.close()
	
	# As a final step, inclued a reference to the config file itself
	dConf['__file__'] = g_sConfPath
	
	return dConf

# ########################################################################## #
# Update sys.path, boiler plate code that has to be re-included in each script
# since config file can change module path

def setModulePath(dConf):
	if 'MODULE_PATH' not in dConf:
		perr(u"Set MODULE_PATH = /dir/containing/das2server_python_module")
		return False	
	
	lDirs = dConf['MODULE_PATH'].split(os.pathsep)
	for sDir in lDirs:
		if os.path.isdir(sDir):
				if sDir not in sys.path:
					sys.path.insert(0, sDir)
		
	return True

# ########################################################################## #
# Writing files #

def _writeFile(sPath, sOutput):
	#perr("Writing: %s"%sPath)
	sDir = dname(sPath)

	if not os.path.isdir(sDir):
		os.makedirs(sDir)

	with open(sPath, 'w') as f:
		f.write(sOutput)

def _writeJsonFile(sPath, dOutput):
	sOutput = json.dumps(dOutput, indent="  ");
	_writeFile(sPath, sOutput)


# ########################################################################## #
# The program needs way better help than the default OptionParser can provide

class MyOptParse(optparse.OptionParser):
	def print_help(self, file=None):
		if file == None:
			file = sys.stdout


		# Help pops before the utility module is loaded, hand code these but be
		# on the lookout for changes.  
		dRep = {
			'das2':'das2.d2t', 'das3':'das3.json', 'das3ws':'das3rt.json', 
			'intern':'internal.json', 'conf':g_sConfPath
		}

		file.write("""
NAME:
   das_srv_sdef - Create a sets of related data source definitions

SYNOPSIS:
   das_srv_sdef [options] FILE1 [FILE2 FILE3 ...]

DESCRIPTION:
   das_srv_sdef defines a data source collection for a server.  A single input
   FILE in either *.dsdf or *.json syntax is parsed to produced the output. The
   output consists of at least four files:

      root/$LOCAL_ID.json          - An grouping of related data sources
      root/$LOCAL_ID/%(das2)s      - A das v2.2 source description 
      root/$LOCAL_ID/%(das3)s     - An HttpStreamSrc catalog object
      root/$LOCAL_ID/%(intern)s - Internal processing instructions

   Other outputs may be added to the basic forms above for real-time support
   and external system compatability.

   The LOCAL_ID value is critical to properly organizing data sources.  It can
   be provided inside source definition files and on the command line:

      localId = Value      DSDF file
      "localId":"Value"    JSON file

      -l ID                Command Line, single item
      -d ROOT_DIR          Command Line, relative to some root directory

   No matter the source, the LOCAL_ID must be usable as a legal relative
   directory name as it defines the path to the data source files from the
   server root.  Typically LOCAL_IDs from a 3-level hierachy that organizes
   sources by mission name, then by instrument name and finally by specific
   data source, but this is merely a convention.

OPTIONS:
   -h, --help  Print this help message and exit
	
   -c FILE, --config=FILE
               Use FILE as the das2server.conf configuration instead of the
               compiled in default.

   -i, --install
               Install the source definition under the datasources directory
               for the server defined by the configuration file.  This is
               not compatable with -o.

   -o DIR, --out-dir=DIR
               Unless source definitions are to be installed (-i), they are
               normally written to the current directory.  Use this option
               to select an alternate, non-install, output directory.

   -d DIR, --dir-to-id DIR
               Useful for importing DSDFs from an existing das2 server.  Do not
               look in DSDFs for a 'localId' property.  Instead assume that the
               ID is defined by the relative path from the directory DIR.  Also
               assume that the filename provides the last component of the
               local ID.

   -l ID, --local-id=ID
               Provide the local ID of the data source via the command line.
               This over-rides any value provided by '-d' or in the source file
               itself.

   -n, --no-catalog
               Don't try to creat or update any catalog files that would lead
               to the source set.

   -W, --no-web-sock
               Even if the source supports realtime operations and $WEBSOCK_URI
               is defined in the server configuration, don't output or link a
               WebSockSrc catalog object.

   -H, --no-hapi 
               Even if the source and the server support the HAPI protocol,
               don't output or link a HAPI v2.0 info object.

   -V, --no-vo Even if the source and the server support the IVOA datalink 
               protocol, don't output or link an IVOA service definition.

   --no-gen    When processing JSON source templates, only expand $include
               sections, don't expand automatically $generate'd definitions.
               This is incompatable with --install

FILES:
   Each das2py-server is defined by a single top-level configuration file. By
   default, configuration data for this program are taken from:
	
      %(conf)s

EXAMPLES:
   1. Processing a das2 DSDF file that has localId keyword defined as 
      'Juno/WAV/Survey' within the file using the command:

         das_srv_sdef survey.dsdf

      will create at least the following files:

         ./root/juno/wav/survey.json 
         ./root/juno/wav/survey/%(das2)s    # If source is das v2.2 compatable
         ./root/juno/wav/survey/%(das3)s  
         ./root/juno/wav/survey/%(das3ws)s # If real-time operations supported

      Other source definitions files may be created for compatability with
      other APIs, depending on the server configuration.

   2. Process a JSON template for the dataset that does not define a local ID
      and install it for use by the server:

         das_srv_sdef -i -l Voyager/1/PWS/Waveform waveform.json

      The resulting data source set will be visible at:

         $SERVER_URL/source/voyager/1/pws/waveform.json

      and the %(das2)s file with be available at:

         $SERVER_URL?server=dsdf&dataset=Voyager/1/PWS/Waveform         

      In the paths above, $SERVER_URL is defined in das2server.conf.

   3. Import all DSDFs for a server in one command using relative file paths to
      define the Local ID:

         ROOT=/var/www/das2srv/datasets
         das_srv_sdef -d $ROOT $(find $ROOT -name "*.dsdf")

SEE ALSO:
   The dsdf format is defined by das2 ICD at DOI: 10.5281/zenodo.3588534

   The json template format is yet to be codified, see examples distributed
   with das2py-server.

"""%dRep)

# ########################################################################## #
def main(argv):
	global das2, U

	sUsage = "das_srv_sdef [options] DATA_SOURCE_FILE"
	psr = MyOptParse(prog="das_srv_sdef", usage="sUsage")

	psr.add_option('-c', '--config', dest="sConfig", default=g_sConfPath)
	psr.add_option(
		'-i', '--install', action="store_true", dest="bInstall", default=False
	)
	sDefRoot = '.'
	psr.add_option('-o', '--out-dir', dest="sRoot", default=sDefRoot)
	psr.add_option(
		'-W', '--no-web-sock', action="store_false", dest='bSocSrc', default=True
	)
	psr.add_option(
		'', '--no-gen', action="store_true", dest="bIncOnly", default=False
	)
	psr.add_option(
		'', '--no-cat', action="store_false", dest="bNewCat", default=True
	)
	psr.add_option('-l','--local-id', dest="sLocalId", default=None)
	psr.add_option('-d','--dir-to-id', dest="sLocalRoot", default=None)

	(opts,lPaths) = psr.parse_args()

	if len(lPaths) < 1:
		perr("No data source file specified, use -h for help.")
		return 13
	
	if (len(lPaths) > 1) and opts.sLocalId:
		perr("Argument '-l' can only be used when processing files one at a time.")
	
	dConf = getConf()
	if dConf == None:
		return 17
		
	# Set the system path
	if not setModulePath(dConf):
		return 18
		
	# Load the das2 module
	#try:
	#	das2 = __import__('das2', globals(), locals(), [], 0)
	#except ImportError as e:
	#	perr("Error importing module 'das2' using %s\r\n: %s\n"%(
	#		str(e), opts.sConfig))
	#	return 19
	
	# Load the das2server.util module
	try:
		mTmp = __import__('das2server', globals(), locals(), ['util'], 0)
	except ImportError as e:
		perr("Error importing module 'das2server' using %s\r\n: %s\n"%(
			str(e), opts.sConfig))
		return 19
	try:
		U = mTmp.util
	except AttributeError:
		perr("Server definition: %s"%opts.sConfig)
		perr('No module named das2server.util under %s\n'%dConf['MODULE_PATH'])
		return 20

	fLog = BufferLog()

	if opts.bSocSrc:
		if 'WEBSOCKET_URI' not in dConf:
			perr('INFO: Set WEBSOCKET_URI in %s to support realtime readers'%opts.sConfig);
			opts.bSocSrc = False

	lLocalSrcIds = [] # Save list of source IDs for second pass work

	sRoot = opts.sRoot
	if opts.bInstall: 
		if sRoot != sDefRoot:
			perr(
				"The install option (-i) and output directory option (-o) "+\
				"conflict.  Use -h for help."
			)
			return 21
		sRoot = dConf['DATASRC_ROOT']

	for sPath in lPaths:

		if not os.path.isfile(sPath):
			perr('File %s does not exist'%sPath)
			return 21

		# If we're named _dirinfo_.dsdf, delay usage
		if bname(sPath) == '_dirinfo_.dsdf': continue

		# Check the input type
		sInType = None
		if sPath.lower().endswith('.dsdf'): sInType = 'dsdf'
		else:
			sInType = 'json'
			if not sPath.lower().endswith('.json'):
				perr("File %s is not recognized, does not end "%sPath+\
					"in one of '.dsdf' or '.json'")
			return 14
		
		if opts.bSocSrc:
			if not U.convdsdf.hasRtSupport(fLog, dConf, sPath):
				#perr('Not web-socket capable, Keyword "realTime" does not evaluate '+\
				#     'to True in %s.'%sPath)
				#return 22
				opts.bSocSrc = False
	
		# If sLocalId defined, use it.  Otherwise get it from the source
		if not opts.sLocalId:

			# Check to see if we are getting local IDs from filesystem paths
			if opts.sLocalRoot:
				n = sPath.find(sLocalRoot)
				if n < 0:
					perr("Local Root %s does not appear in source path %s"%(
						opts.sLocalRoot, sPath
					))
				sLocalId = sPath[n+1:].replace(".dsdf",'').replace(".json",'')
				sLocalId = sLocalId.strip(os.sep)

			else:
				sLocalId = U.convdsdf.getLocalId(fLog, dConf, sPath)
				if not sLocalId:
					perr("Local ID not defined in %s nor provided via the command line"%sPath)
					return 22
		else:
			sLocalId = opts.sLocalId

		dPaths = U.catalog.sourceFiles(sRoot, sLocalId)
	
		perr("Input:  %s %s"%(opts.sConfig, sPath))
		lOutput = []
		try:
			if sInType == 'dsdf':
				lOutput.append(dPaths['das3'])
				sFormAction = '%s/data'
				_writeJsonFile(lOutput[-1], U.convdsdf.makeGetSrc(fLog, dConf, sPath, sLocalId))
				
				if opts.bSocSrc:
					lOutput.append(dPaths['das3rt'])
					_writeJsonFile(lOutput[-1], U.convdsdf.makeSockSrc(fLog, dConf, sPath, sLocalId))
				
				lOutput.append(dPaths['intern'])	
				_writeJsonFile(lOutput[-1], U.convdsdf.makeInternal(fLog, dConf, sPath))
				
				lOutput.append(dPaths['das2'])
				_writeFile(lOutput[-1], U.convdsdf.makeD2t(fLog, dConf, sPath))
				

				sDas1 = U.convdsdf.makeDas1(fLog, dConf, sPath)
				if sDas1: 
					lOutput.append(dPaths['das1'])
					_writeFile(lOutput[-1], sDas1)
					
			else:
				lOutput.append(dPaths['das3'])
				_writeJsonFile(lOutput[-1], U.convjson.makeFedCat(fLog, dConf, sPath))

				lOutput.append(dPaths['intern'])
				_writeJsonFile(lOutput[-1], U.convjson.makeDas3Int(fLog, dConf, sPath))

				lOutput.append(dPaths['das2'])
				_writeFile(lOutput[-1], U.convjson.makeD2t(fLog, dConf, sPath))
	
			# Read the sources you've written and update the collection
			lOutput.append(dPaths['set'])
			U.catalog.makeCollection(dConf, sLocalId, lOutput, lOutput[-1])
	
			perr("Source Def: %s"%("\n            ".join(lOutput)))
	
		except U.errors.ServerError as e:
			perr('%s'%fLog.getvalue())
			perr('ERROR: %s'%str(e))
			return 22

		lLocalSrcIds.append(sLocalId)

	# Handle catalog updates if desired...
	if not opts.bNewCat:
		return 0

	# Write in the _dirinfo_.dsdf entries
	for sPath in lPaths:

		if bname(sPath) != '_dirinfo_.dsdf': continue

		perr("Input:  %s %s"%(opts.sConfig, sPath))

		# If sLocalId defined, use it.  Otherwise get it from the source
		if not opts.sLocalId:

			# Check to see if we are getting local IDs from filesystem paths
			if opts.sLocalRoot:
				sDir = dname(sPath)
				n = sDir.find(sLocalRoot)
				if n < 0:
					perr("Local Root %s does not appear in source path %s"%(
						opts.sLocalRoot, sPath
					))
				sLocalId = sDir[n+1:].replace(os.sep, '/')
			else:
				sLocalId = U.convdsdf.getLocalId(fLog, dConf, sPath)
				if not sLocalId:
					perr("Local ID not defined in %s nor provided via the command line"%sPath)
					return 22
		else:
			sLocalId = opts.sLocalId

		sDesc = U.convdsdf.getDescription(fLog, dConf, sPath)
		if sDesc:
			U.catalog.addCatTitle(dConf, sRoot, sLocalId, sDesc)


	# Walk backwards up the tree updating catalogs as you go
	for sLocalId in lLocalSrcIds:
		lUpdates = U.catalog.updateFromSrc(dConf, sRoot, sLocalId)
		if not lUpdates:
			return 7
		perr("Node Update: %s"%("\n             ".join(lUpdates)))

	# Recreate the summary listings
	lWrote = U.catalog.updateLists(dConf, sRoot)
	if not lWrote:
		return 8
	perr("List Update: %s"%("\n             ".join(lWrote)))
		
	return 0

# ########################################################################## #
if __name__ == '__main__':
	main(sys.argv)
